# **Machine Learning**

## ğŸ“Œ Como a mÃ¡quina aprende?

> **Dados â†’ Processamento â†’ Modelo**

---

## ğŸ“Š Tipos de dados

Em Machine Learning, os dados mais comuns sÃ£o **tabulares (tabelas)**.  
Essas tabelas possuem trÃªs elementos principais:

- **Atributos:** colunas dos dados tabulares.  
- **InstÃ¢ncias:** linhas (exceto o cabeÃ§alho).  
- **Classes:** objetivo ou rÃ³tulo que se deseja prever.  

---

## ğŸ§© Tarefas em Machine Learning

| **ClassificaÃ§Ã£o** | **RegressÃ£o** | **Agrupamento** | **Regras de AssociaÃ§Ã£o** |
| ----------------- | ------------- | ---------------- | ------------------------- |
| Predizer um atributo especial chamado â€œclasseâ€. | Prever valores numÃ©ricos contÃ­nuos. | Agrupar dados com base em caracterÃ­sticas matemÃ¡ticas. | Identificar relaÃ§Ãµes frequentes entre itens de um conjunto. |
| Supervisionado | Supervisionado | NÃ£o supervisionado | Supervisionado ou nÃ£o supervisionado |

---

## ğŸ“ Medindo o desempenho do modelo

- **Treino:** algoritmo processa os dados e gera um modelo.  
- **ValidaÃ§Ã£o:** dados usados para ajuste fino do modelo.  
- **Teste:** dados usados para avaliar a performance final.  

---

## âš™ï¸ TÃ©cnicas de treino

- **Hold-out:** separa 70% dos dados para treino e 30% para teste.  
- **ValidaÃ§Ã£o Cruzada (k-fold):** divide os dados em *k* subconjuntos; cada subconjunto Ã© usado como teste uma vez.  
- **Leave-one-out:** variaÃ§Ã£o da validaÃ§Ã£o cruzada, mas deixa apenas **1 instÃ¢ncia** de fora por vez.  
- **Subamostragem:** semelhante ao hold-out, mas repetido vÃ¡rias vezes com amostras aleatÃ³rias.  

---

## ğŸ“ GeneralizaÃ§Ã£o x Overfitting x Underfitting

- **GeneralizaÃ§Ã£o (ideal):** bom desempenho tanto em treino quanto em produÃ§Ã£o.  
- **Overfitting (superajuste):** modelo aprende *demais* os dados de treino e nÃ£o generaliza bem.  
  - **Causas:** poucos dados, modelo complexo, inconsistÃªncia nos dados, mÃ¡ seleÃ§Ã£o de atributos, falta de validaÃ§Ã£o cruzada.  
- **Underfitting (subajuste):** modelo muito simples, incapaz de capturar padrÃµes.  
  - **Causas:** simplicidade excessiva, poucos dados, mÃ¡ seleÃ§Ã£o de atributos, hiperparÃ¢metros mal ajustados.  

---

## ğŸ§® Matriz de ConfusÃ£o (ClassificaÃ§Ã£o)

|  | Classe real positiva | Classe real negativa |
| - | ------------------ | --------------------- |
| **Classe prevista positiva** | Verdadeiros Positivos (VP) | Falsos Positivos (FP) |
| **Classe prevista negativa** | Falsos Negativos (FN) | Verdadeiros Negativos (VN) |

### MÃ©tricas principais

- **AcurÃ¡cia:** `(VP + VN) / (VP + VN + FP + FN)`  
- **PrecisÃ£o:** `VP / (VP + FP)`  
- **Recall (Sensibilidade):** `VP / (VP + FN)`  
- **Especificidade:** `VN / (VN + FP)`  
- **F1 Score:** `2 Ã— (PrecisÃ£o Ã— Recall) / (PrecisÃ£o + Recall)`  

---

## ğŸ“ˆ ROC e AUC

A **ROC (Receiver Operating Characteristic)** Ã© uma curva usada para avaliar classificadores binÃ¡rios.  
A mÃ©trica **AUC-ROC** mede a Ã¡rea sob essa curva:

- Valor entre **0 e 1**.  
- Quanto mais prÃ³ximo de **1**, melhor a performance do classificador.  

---

## ğŸ“ MÃ©tricas de Erro

Usadas principalmente em **regressÃ£o** para avaliar a diferenÃ§a entre valores previstos (Å·áµ¢) e valores reais (yáµ¢).

- **ME (Mean Error):**  
  MÃ©dia simples dos erros. Pode ser positiva ou negativa (depende da escala e direÃ§Ã£o do erro).  
  `FÃ³rmula: ME = (Î£ (yáµ¢ - Å·áµ¢)) / n`

- **MAE (Mean Absolute Error):**  
  MÃ©dia do valor absoluto dos erros. Mede o erro mÃ©dio sem considerar sinal. (Dependente da escala)  
  `FÃ³rmula: MAE = (Î£ |yáµ¢ - Å·áµ¢|) / n`

- **MSE (Mean Squared Error):**  
  MÃ©dia dos erros elevados ao quadrado. Penaliza mais erros grandes. (Dependente da escala)  
  `FÃ³rmula: MSE = (Î£ (yáµ¢ - Å·áµ¢)Â²) / n`

- **RMSE (Root Mean Squared Error):**  
  Raiz quadrada do MSE. InterpretaÃ§Ã£o mais intuitiva porque retorna Ã  mesma unidade dos dados originais. (Dependente da escala)  
  `FÃ³rmula: RMSE = âˆš((Î£ (yáµ¢ - Å·áµ¢)Â²) / n)`

- **MPE (Mean Percentage Error):**  
  MÃ©dia percentual dos erros em relaÃ§Ã£o aos valores reais. Pode ser positiva ou negativa, o que dificulta a interpretaÃ§Ã£o em alguns casos. (nÃ£o depende de escala)
  `FÃ³rmula: MPE = (Î£ ((yáµ¢ - Å·áµ¢) / yáµ¢)) / n Ã— 100`

- **MAPE (Mean Absolute Percentage Error):**  
  MÃ©dia percentual **absoluta** dos erros. Ã‰ a mÃ©trica mais usada para avaliar erro relativo, pois sempre retorna valores positivos. (nÃ£o depende de escala)
  `FÃ³rmula: MAPE = (Î£ (|yáµ¢ - Å·áµ¢| / |yáµ¢|)) / n Ã— 100`

- **RÂ² (Coeficiente de DeterminaÃ§Ã£o):**  
  Mede o quanto o modelo explica da variabilidade dos dados (varia de 0 a 1).  
  `FÃ³rmula: RÂ² = 1 - (Î£ (yáµ¢ - Å·áµ¢)Â² / Î£ (yáµ¢ - È³)Â²)`

---

## ğŸ”  Categorical Encoding

Muitos algoritmos de Machine Learning nÃ£o conseguem lidar diretamente com variÃ¡veis **categÃ³ricas** (ex.: cores, nomes, cidades).  
Por isso, Ã© necessÃ¡rio transformÃ¡-las em **nÃºmeros** para que os modelos possam processar os dados.  
Esse processo Ã© chamado de **codificaÃ§Ã£o categÃ³rica (categorical encoding).**

Existem vÃ¡rias tÃ©cnicas, sendo as mais comuns:

---

### ğŸ¯ Label Encoding

O **Label Encoding** transforma cada categoria em um nÃºmero inteiro.  

Exemplo:  

| Cor     | CodificaÃ§Ã£o |
|---------|-------------|
| Vermelho | 0 |
| Azul     | 1 |
| Verde    | 2 |

âœ… **Vantagens:**  

- Simples e rÃ¡pido.  
- NÃ£o aumenta o nÃºmero de colunas.  

âš ï¸ **Desvantagens:**

- Introduz uma ordem numÃ©rica que pode **nÃ£o existir** (o modelo pode â€œacharâ€ que Verde > Azul, por exemplo).  
- Pode causar problemas em algoritmos baseados em distÃ¢ncia ou regressÃ£o.  

ğŸ“Œ **Quando usar:**

- Em variÃ¡veis ordinais (ex.: pequeno, mÃ©dio, grande).  

---

### ğŸŸ¦ One-Hot Encoding

O **One-Hot Encoding** cria uma nova coluna para cada categoria, preenchendo com **0 ou 1** para indicar a presenÃ§a da categoria.  

Exemplo:

| Cor     | Azul | Verde | Vermelho |
|---------|------|-------|----------|
| Vermelho | 0 | 0 | 1 |
| Azul     | 1 | 0 | 0 |
| Verde    | 0 | 1 | 0 |

âœ… **Vantagens:**

- Evita a falsa ordem numÃ©rica.  
- Funciona bem na maioria dos algoritmos.  

âš ï¸ **Desvantagens:**

- Aumenta o nÃºmero de colunas (especialmente se houver muitas categorias â†’ problema de *dimensionalidade*).  
- Pode deixar o modelo mais lento com dados de alta cardinalidade.  

ğŸ“Œ **Quando usar:**

- Em variÃ¡veis nominais (sem ordem natural, ex.: cidade, cor, produto).  

---

ğŸ‘‰ AlÃ©m dessas, existem outras tÃ©cnicas (como **Target Encoding, Binary Encoding, Frequency Encoding**) usadas em casos mais especÃ­ficos, mas **Label** e **One-Hot** sÃ£o as bases que vocÃª mais vai encontrar no dia a dia.

---

## ğŸ”¢ TransformaÃ§Ã£o de Dados NumÃ©ricos

Em Machine Learning, muitas vezes lidamos com variÃ¡veis em **escalas diferentes**.  
Exemplo: uma coluna pode representar **idade (anos)** e outra **salÃ¡rio (R$)**.  

Se nÃ£o tratarmos isso, as variÃ¡veis com valores maiores podem **dominar** o treinamento do modelo, contribuindo de forma **desbalanceada**.

AlÃ©m disso, em algoritmos que usam **Gradient Descent**, o escalonamento ajuda a **convergir mais rapidamente** para o mÃ­nimo local/global.

---

### âš–ï¸ PadronizaÃ§Ã£o (Z-score)

A **PadronizaÃ§Ã£o** transforma os dados para que tenham **mÃ©dia = 0** e **desvio padrÃ£o = 1**.  
Ou seja, cada valor passa a representar quantos desvios padrÃ£o estÃ¡ distante da mÃ©dia.

ğŸ“Œ **FÃ³rmula:** z = (x - Î¼) / Ïƒ

- `x`: valor original  
- `Î¼`: mÃ©dia da variÃ¡vel  
- `Ïƒ`: desvio padrÃ£o da variÃ¡vel  

âœ… Bom para dados que seguem (ou quase seguem) uma **distribuiÃ§Ã£o normal (gaussiana)**.  

---

### ğŸ“ NormalizaÃ§Ã£o (Min-Max Scaling)

A **NormalizaÃ§Ã£o** ajusta os dados para um intervalo fixo, geralmente **[0, 1]**.  
Dessa forma, o menor valor vira **0** e o maior vira **1**, com os demais proporcionais.

ğŸ“Œ **FÃ³rmula:** x' = (x - x_min) / (x_max - x_min)

- `x`: valor original  
- `x_min`: menor valor da variÃ¡vel  
- `x_max`: maior valor da variÃ¡vel  

âœ… Ãštil quando os dados nÃ£o seguem distribuiÃ§Ã£o normal ou quando queremos manter a escala proporcional.  

âš ï¸ Pode ser **fortemente impactada por outliers**.  

---

### ğŸš¨ Outliers

**Outliers** sÃ£o valores que fogem muito do padrÃ£o dos dados (ex.: salÃ¡rio de R$ 1.000.000 em um conjunto onde a mÃ©dia Ã© R$ 3.000).  

- Podem **distorter padronizaÃ§Ã£o e normalizaÃ§Ã£o**, puxando os dados para extremos.  
- EstratÃ©gias para lidar com outliers:  
  - RemoÃ§Ã£o (quando sÃ£o erros claros de coleta).  
  - TransformaÃ§Ãµes estatÃ­sticas (log, raiz quadrada).  
  - Escalonamento robusto (ex.: **RobustScaler** no sklearn, que usa mediana em vez da mÃ©dia).  

---

ğŸ“Œ **Resumo:**

- **PadronizaÃ§Ã£o (Z-score):** melhor quando os dados tÃªm distribuiÃ§Ã£o prÃ³xima da normal.
- **NormalizaÃ§Ã£o (Min-Max):** melhor quando queremos dados em uma escala fixa, mas cuidado com outliers.  
- **Outliers:** precisam ser analisados, pois podem comprometer ambos os mÃ©todos.

---

## ğŸ”€ Clustering (Agrupamento)

O **Clustering** Ã© uma tÃ©cnica de aprendizado **nÃ£o supervisionado** cujo objetivo Ã© **agrupar dados semelhantes em conjuntos**, chamados de **clusters**.  
Diferente de mÃ©todos supervisionados, o clustering nÃ£o usa rÃ³tulos (labels): o prÃ³prio algoritmo encontra a **estrutura** e os **padrÃµes** escondidos nos dados.

---

### ğŸ“Œ Tipos de Agrupamento

Existem diferentes abordagens de agrupamento, dependendo de como os dados sÃ£o tratados:

- **Agrupamento completo:**  
  Cada instÃ¢ncia Ã© atribuÃ­da a **apenas um cluster**. Nenhum dado fica de fora.  

- **Agrupamento parcial:**  
  Uma instÃ¢ncia pode pertencer a mais de um cluster, ou atÃ© nÃ£o pertencer a nenhum. Ãštil quando existe **sobreposiÃ§Ã£o** entre grupos.  

- **Modelo difuso (Fuzzy Clustering):**  
  Em vez de atribuir rigidamente cada ponto a um cluster, cada instÃ¢ncia recebe um **grau de pertinÃªncia** (ex.: 80% no Cluster A, 20% no Cluster B).  

- **Modelo hierÃ¡rquico:**  
  Cria uma estrutura em forma de Ã¡rvore (*dendrograma*), permitindo **subgrupos dentro de clusters maiores**.  
  NÃ£o exige definir o nÃºmero de clusters antecipadamente.  

AlÃ©m disso, os algoritmos podem lidar com os dados de duas formas:  

- **Agrupam todos os pontos:** Ex.: **K-Means**.
- **Podem deixar ruÃ­dos de fora:** Ex.: **DBSCAN**, que identifica pontos que nÃ£o pertencem a nenhum cluster.  

---

### âš™ï¸ Algoritmos de Clustering

- **K-Means:**  
  - Divide os dados em **k clusters**.  
  - Cada ponto Ã© atribuÃ­do ao centroide mais prÃ³ximo.  
  - Iterativamente, os centroides sÃ£o recalculados.  
  - âš ï¸ Ã‰ necessÃ¡rio definir o valor de **k** previamente.  

- **K-Medoids:**  
  - Semelhante ao K-Means, mas usa um **ponto real do cluster** como representante (medoide).  
  - âœ… Mais robusto a ruÃ­dos e *outliers*.  

- **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**  
  - Agrupa pontos com base na **densidade local**.  
  - âœ… Detecta clusters de formatos arbitrÃ¡rios.  
  - âœ… Identifica automaticamente **outliers** como ruÃ­dos.  
  - âœ… NÃ£o precisa definir **k** previamente.  

---

## ğŸ”— Regras de AssociaÃ§Ã£o

As **Regras de AssociaÃ§Ã£o** sÃ£o tÃ©cnicas de aprendizado nÃ£o supervisionado usadas para encontrar **relaÃ§Ãµes frequentes entre itens** em grandes bases de dados.  

ğŸ“Œ Exemplo clÃ¡ssico: anÃ¡lise de cestas de supermercado.  
> â€œQuem compra **pÃ£o** tambÃ©m costuma comprar **leite**.â€  

Essas regras sÃ£o expressas na forma: Se {Item A} â†’ EntÃ£o {Item B}

---

### ğŸ“Š MÃ©tricas principais

- **Suporte:**  
  Mede a **frequÃªncia** com que os itens aparecem juntos no conjunto de dados.  

- **ConfianÃ§a:**  
  Mede a **probabilidade** de comprar o item B, dado que o item A jÃ¡ foi comprado.  
  - FÃ³rmula:  

    ```ConfianÃ§a(A â†’ B) = Suporte(A âˆ§ B) / Suporte(A)```

  - Ex.: Uma confianÃ§a de **80%** significa que, em 80% das vezes em que A foi comprado, B tambÃ©m foi.  

- **Lift (ElevaÃ§Ã£o):**  
  Mede se a relaÃ§Ã£o Ã© mais forte do que o esperado por acaso.  
  - FÃ³rmula:  

    ```Lift(A â†’ B) = ConfianÃ§a(A â†’ B) / Suporte(B)```

  - InterpretaÃ§Ã£o:  
    - **Lift > 1:** AssociaÃ§Ã£o positiva (forte).  
    - **Lift = 1:** IndependÃªncia.  
    - **Lift < 1:** AssociaÃ§Ã£o negativa (a presenÃ§a de A reduz chance de B).  

---

## ğŸ›’ Algoritmos: Apriori e FP-Growth

- **Apriori:**  
  - Gera conjuntos de itens frequentes a partir do suporte mÃ­nimo.  
  - ConstrÃ³i regras de associaÃ§Ã£o com base nesses conjuntos.  
  - âœ… Simples e intuitivo, mas pode ser **computacionalmente caro** em grandes bases.  

- **FP-Growth (Frequent Pattern Growth):**  
  - Mais eficiente que o Apriori.  
  - Usa uma estrutura chamada **FP-tree** para compactar os dados.  
  - âœ… EscalÃ¡vel e rÃ¡pido em grandes volumes de transaÃ§Ãµes.  
